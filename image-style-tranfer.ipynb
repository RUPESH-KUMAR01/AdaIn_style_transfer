{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":5127,"databundleVersionId":868727,"sourceType":"competition"},{"sourceId":2598787,"sourceType":"datasetVersion","datasetId":1573501},{"sourceId":250386,"sourceType":"modelInstanceVersion","modelInstanceId":214013,"modelId":235688},{"sourceId":250391,"sourceType":"modelInstanceVersion","modelInstanceId":214017,"modelId":235692},{"sourceId":250663,"sourceType":"modelInstanceVersion","modelInstanceId":214261,"modelId":235934},{"sourceId":255521,"sourceType":"modelInstanceVersion","modelInstanceId":218450,"modelId":240177},{"sourceId":258006,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":220496,"modelId":242274}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport zipfile\nimport os\nimport shutil\nfrom pathlib import Path\nimport torch.nn as nn","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Implementing Paper Image Style Transfer\n\nThis code is for reacreating the [Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization](https://arxiv.org/abs/1703.06868)\n\n\nI will try to re-implement the things they followed with the Painter by Numbers Dataset.\n\nLater in the same code I will try distributed training","metadata":{}},{"cell_type":"markdown","source":"One more Interesting thing in this paper is I dont need a ground truth. All we need is a bunch Image Pairs","metadata":{}},{"cell_type":"code","source":"img_size = 512  # Resize all images to this size\nbatch_size = 8\nori_lr = 1e-4\nmax_iter = 70000\ncontent_weight = 1.0\nstyle_weight = 10.0\nsave_iter = 10000\nlr_decay=5e-5\n\ncontent_zip = \"/kaggle/input/painter-by-numbers/train_1.zip\"\nstyle_zip = \"/kaggle/input/painter-by-numbers/test.zip\"\n\ncontent_dir_name = \"content\"\nstyle_dir_name = \"style\"\n\ncontent_dir = r\"/kaggle/input/coco-2014-dataset-for-yolov3/coco2014/images/test2014\"\nstyle_dir = os.path.join(os.getcwd(), style_dir_name)\nsave_dir=Path(Path.cwd() / 'save')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Get the images out of zip file\n\ndef extract_images_from_zip(zip_path,output_dir):\n    # Create output directory if it doesn't exist\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Open and extract the ZIP file\n    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n        # Get list of all files in ZIP\n        file_list = zip_ref.namelist() \n\n        # Filter image files (common image extensions)\n        image_extensions = ('.jpg', '.jpeg', '.png', '.gif', '.bmp')\n        image_files = [f for f in file_list if f.lower().endswith(image_extensions)]\n        \n        # Extract only image files\n        for image_file in image_files:\n            zip_ref.extract(image_file, output_dir)\n            extracted_path = os.path.join(output_dir, image_file)\n            final_path = os.path.join(output_dir, os.path.basename(image_file))\n            os.rename(extracted_path, final_path)\n\n    print(f\"\\nExtraction complete! Images saved to: {output_dir}\")\n\n# extract_images_from_zip(content_zip,content_dir_name)\nextract_images_from_zip(style_zip,style_dir_name)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#transforms\nfrom torchvision import transforms\ndef train_transform():\n    transform_list = [\n        transforms.Resize(size=(512, 512)),\n        transforms.RandomCrop(256),\n        transforms.ToTensor()\n    ]\n    return transforms.Compose(transform_list)\ncontent_tf=train_transform()\nstyle_tf=train_transform()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#first we need to process the Dataset using torch.utils.Dataset\nimport glob\nimport torch\nfrom torch.utils.data import Dataset\nfrom PIL import Image\n\nclass Style_Dataset(Dataset):\n    def __init__(self, img_dir, transform):\n        self.transform = transform\n        self.img_files = [f for f in glob.glob(f\"{img_dir}/*\") if os.path.isfile(f)]  # Only use valid image files\n        img_path=\"/kaggle/input/coco-2014-dataset-for-yolov3/coco2014/images/train2014/COCO_train2014_000000000009.jpg\"\n        self.extra_image=Image.open(img_path).convert(\"RGB\")\n        self.extra_image=self.transform(self.extra_image)\n    def __getitem__(self, idx):\n        img_path = self.img_files[idx]\n        try:\n            with Image.open(img_path) as img:\n                if img.size[0] * img.size[1] > 89_478_485:  # Pillow's safety limit\n                    print(f\"Skipping large image: {img_path}, size: {img.size}\")\n                    return self.extra_image\n                img = img.convert(\"RGB\")\n                img = self.transform(img)\n                return img\n        except Exception as e:\n            print(f\"Skipping corrupt file: {img_path}, Error: {e}\")\n            return self.extra_image\n\n    def __len__(self):\n        return len(self.img_files)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\ndef collate_fn(batch):\n    batch = [img for img in batch if img is not None]  # Remove None values\n    if len(batch) == 0:  \n        return None  # Handle empty batch case\n    return torch.stack(batch)  # Stack images into a tensor batch","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#testing the dataset\nimport matplotlib.pyplot as plt\ncontent_dataset = Style_Dataset(content_dir,content_tf)\nstyle_dataset = Style_Dataset(style_dir,style_tf)\ncontent_img= content_dataset[0]\ncontent_img = content_img.permute(1, 2, 0).numpy() \nstyle_img=style_dataset[0]\nstyle_img=style_img.permute(1, 2, 0).numpy()\nplt.figure(figsize=(10, 5))\n\nplt.subplot(1, 2, 1)\nplt.imshow(content_img)\nplt.title(\"Content Image\")\nplt.axis(\"off\")\n\nplt.subplot(1, 2, 2)\nplt.imshow(style_img)\nplt.title(\"Style Image\")\nplt.axis(\"off\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(content_dataset)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Infinite Sampler which will be used in the dataloader\nfrom torch.utils.data.sampler import Sampler\n\n# An Infinite Sampler is a type of PyTorch Sampler that \n# provides an endless stream of shuffled data indices. Unlike\n# standard samplers that stop after one full pass through the dataset\n# ,an infinite sampler continuously reshuffles and reuses the dataset\n# without explicitly restarting an epoch.\n\ndef InfiniteSampler(n):\n    order = np.random.permutation(n)\n    i = 0\n    while True:\n        yield order[i]\n        i += 1\n        if i >= n:\n            order = np.random.permutation(n)\n            i = 0\n\nclass InfiniteSamplerWrapper(Sampler):\n    def __init__(self, data_source):\n        self.num_samples = len(data_source)\n\n    def __iter__(self):\n        return iter(InfiniteSampler(self.num_samples))\n\n    def __len__(self):\n        return 2**31  # Large value to indicate infinity\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#dataloader\nfrom torch.utils.data import DataLoader\n# we need two loaders one for content and other for style \ncontent_iter=iter(DataLoader(content_dataset,\n                          batch_size=batch_size,\n                          sampler=InfiniteSamplerWrapper(content_dataset),\n                            collate_fn=collate_fn))\nstyle_iter=iter(DataLoader(style_dataset,\n                        batch_size=batch_size,\n                        sampler=InfiniteSamplerWrapper(style_dataset),\n                          collate_fn=collate_fn))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Adaptive Instance Normalization\ndef calc_mean_std(feat, eps=1e-5):\n    # eps is a small value added to the variance to avoid divide-by-zero.\n    size = feat.size()\n    assert (len(size) == 4)\n    N, C = size[:2]\n    feat_var = feat.view(N, C, -1).var(dim=2) + eps\n    feat_std = feat_var.sqrt().view(N, C, 1, 1)\n    feat_mean = feat.view(N, C, -1).mean(dim=2).view(N, C, 1, 1)\n    return feat_mean, feat_std\n\n\ndef adain(content_feat, style_feat):\n    assert (content_feat.size()[:2] == style_feat.size()[:2])\n    size = content_feat.size()\n    style_mean, style_std = calc_mean_std(style_feat)\n    content_mean, content_std = calc_mean_std(content_feat)\n\n    normalized_feat = (content_feat - content_mean.expand(\n        size)) / content_std.expand(size)\n    return normalized_feat * style_std.expand(size) + style_mean.expand(size)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#vgg-16 Structure\nvgg = nn.Sequential(\n    nn.Conv2d(3, 3, (1, 1)),\n    nn.ReflectionPad2d((1, 1, 1, 1)),\n    nn.Conv2d(3, 64, (3, 3)),\n    nn.ReLU(),  # relu1-1\n    nn.ReflectionPad2d((1, 1, 1, 1)),\n    nn.Conv2d(64, 64, (3, 3)),\n    nn.ReLU(),  # relu1-2\n    nn.MaxPool2d((2, 2), (2, 2), (0, 0), ceil_mode=True),\n    nn.ReflectionPad2d((1, 1, 1, 1)),\n    nn.Conv2d(64, 128, (3, 3)),\n    nn.ReLU(),  # relu2-1\n    nn.ReflectionPad2d((1, 1, 1, 1)),\n    nn.Conv2d(128, 128, (3, 3)),\n    nn.ReLU(),  # relu2-2\n    nn.MaxPool2d((2, 2), (2, 2), (0, 0), ceil_mode=True),\n    nn.ReflectionPad2d((1, 1, 1, 1)),\n    nn.Conv2d(128, 256, (3, 3)),\n    nn.ReLU(),  # relu3-1\n    nn.ReflectionPad2d((1, 1, 1, 1)),\n    nn.Conv2d(256, 256, (3, 3)),\n    nn.ReLU(),  # relu3-2\n    nn.ReflectionPad2d((1, 1, 1, 1)),\n    nn.Conv2d(256, 256, (3, 3)),\n    nn.ReLU(),  # relu3-3\n    nn.ReflectionPad2d((1, 1, 1, 1)),\n    nn.Conv2d(256, 256, (3, 3)),\n    nn.ReLU(),  # relu3-4\n    nn.MaxPool2d((2, 2), (2, 2), (0, 0), ceil_mode=True),\n    nn.ReflectionPad2d((1, 1, 1, 1)),\n    nn.Conv2d(256, 512, (3, 3)),\n    nn.ReLU(),  # relu4-1, this is the last layer used\n    nn.ReflectionPad2d((1, 1, 1, 1)),\n    nn.Conv2d(512, 512, (3, 3)),\n    nn.ReLU(),  # relu4-2\n    nn.ReflectionPad2d((1, 1, 1, 1)),\n    nn.Conv2d(512, 512, (3, 3)),\n    nn.ReLU(),  # relu4-3\n    nn.ReflectionPad2d((1, 1, 1, 1)),\n    nn.Conv2d(512, 512, (3, 3)),\n    nn.ReLU(),  # relu4-4\n    nn.MaxPool2d((2, 2), (2, 2), (0, 0), ceil_mode=True),\n    nn.ReflectionPad2d((1, 1, 1, 1)),\n    nn.Conv2d(512, 512, (3, 3)),\n    nn.ReLU(),  # relu5-1\n    nn.ReflectionPad2d((1, 1, 1, 1)),\n    nn.Conv2d(512, 512, (3, 3)),\n    nn.ReLU(),  # relu5-2\n    nn.ReflectionPad2d((1, 1, 1, 1)),\n    nn.Conv2d(512, 512, (3, 3)),\n    nn.ReLU(),  # relu5-3\n    nn.ReflectionPad2d((1, 1, 1, 1)),\n    nn.Conv2d(512, 512, (3, 3)),\n    nn.ReLU()  # relu5-4\n)\nvgg.load_state_dict(torch.load(\"/kaggle/input/vgg_normalised/pytorch/default/1/vgg_normalised.pth\"))\nvgg = nn.Sequential(*list(vgg.children())[:31])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"decoder = nn.Sequential(\n    nn.ReflectionPad2d((1, 1, 1, 1)),\n    nn.Conv2d(512, 256, (3, 3)),\n    nn.ReLU(),\n    nn.Upsample(scale_factor=2, mode='nearest'),\n    nn.ReflectionPad2d((1, 1, 1, 1)),\n    nn.Conv2d(256, 256, (3, 3)),\n    nn.ReLU(),\n    nn.ReflectionPad2d((1, 1, 1, 1)),\n    nn.Conv2d(256, 256, (3, 3)),\n    nn.ReLU(),\n    nn.ReflectionPad2d((1, 1, 1, 1)),\n    nn.Conv2d(256, 256, (3, 3)),\n    nn.ReLU(),\n    nn.ReflectionPad2d((1, 1, 1, 1)),\n    nn.Conv2d(256, 128, (3, 3)),\n    nn.ReLU(),\n    nn.Upsample(scale_factor=2, mode='nearest'),\n    nn.ReflectionPad2d((1, 1, 1, 1)),\n    nn.Conv2d(128, 128, (3, 3)),\n    nn.ReLU(),\n    nn.ReflectionPad2d((1, 1, 1, 1)),\n    nn.Conv2d(128, 64, (3, 3)),\n    nn.ReLU(),\n    nn.Upsample(scale_factor=2, mode='nearest'),\n    nn.ReflectionPad2d((1, 1, 1, 1)),\n    nn.Conv2d(64, 64, (3, 3)),\n    nn.ReLU(),\n    nn.ReflectionPad2d((1, 1, 1, 1)),\n    nn.Conv2d(64, 3, (3, 3)),\n)\n# decoder.load_state_dict(torch.load(\"/kaggle/input/decoder_for_image_stle_transfer/pytorch/default/1/decoder.pth\"))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Entire model with encoder and decoder\nimport torch.optim.swa_utils as swa_utils\n\nclass network(nn.Module):\n    def __init__(self, encoder, decoder):\n        super(network, self).__init__()\n        enc_layers = list(encoder.children())\n        self.enc_1 = nn.Sequential(*enc_layers[:4])  # input -> relu1_1\n        self.enc_2 = nn.Sequential(*enc_layers[4:11])  # relu1_1 -> relu2_1\n        self.enc_3 = nn.Sequential(*enc_layers[11:18])  # relu2_1 -> relu3_1\n        self.enc_4 = nn.Sequential(*enc_layers[18:31])  # relu3_1 -> relu4_1\n        self.decoder = decoder\n        self.mse_loss = nn.MSELoss()\n\n        # fix the encoder\n        for name in ['enc_1', 'enc_2', 'enc_3', 'enc_4']:\n            for param in getattr(self, name).parameters():\n                param.requires_grad = False\n\n    # extract relu1_1, relu2_1, relu3_1, relu4_1 from input image\n    def encode_with_intermediate(self, input):\n        results = [input]\n        for i in range(4):\n            func = getattr(self, 'enc_{:d}'.format(i + 1))\n            results.append(func(results[-1]))\n        return results[1:]\n\n    # extract relu4_1 from input image\n    def encode(self, input):\n        for i in range(4):\n            input = getattr(self, 'enc_{:d}'.format(i + 1))(input)\n        return input\n\n    def calc_content_loss(self, input, target):\n        assert (input.size() == target.size())\n        assert (target.requires_grad is False)\n        return self.mse_loss(input, target)\n\n    def calc_style_loss(self, input, target):\n        assert (input.size() == target.size())\n        assert (target.requires_grad is False)\n        input_mean, input_std = calc_mean_std(input)\n        target_mean, target_std = calc_mean_std(target)\n        return self.mse_loss(input_mean, target_mean) + \\\n               self.mse_loss(input_std, target_std)\n\n    def forward(self, content, style, alpha=1.0):\n        assert 0 <= alpha <= 1\n        style_feats = self.encode_with_intermediate(style)\n        content_feat = self.encode(content)\n        t = adain(content_feat, style_feats[-1])\n        t = alpha * t + (1 - alpha) * content_feat\n\n        g_t = self.decoder(t)\n        g_t_feats = self.encode_with_intermediate(g_t)\n\n        loss_c = self.calc_content_loss(g_t_feats[-1], t)\n        loss_s = self.calc_style_loss(g_t_feats[0], style_feats[0])\n        for i in range(1, 4):\n            loss_s += self.calc_style_loss(g_t_feats[i], style_feats[i])\n        return loss_c, loss_s","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#initialize the model\ndecoder = decoder.to(device)\nvgg=vgg.to(device)\nmodel=network(vgg,decoder).to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#optimizer\nimport torch.optim.lr_scheduler as lr_scheduler\n\noptimizer = torch.optim.Adam(model.decoder.parameters(), lr=ori_lr)\ndef adjust_learning_rate(optimizer, iteration_count):\n    \"\"\"Imitating the original implementation\"\"\"\n    lr = ori_lr / (1.0 + lr_decay * iteration_count)\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Store losses for visualization\ncontent_losses = []\nstyle_losses = []\ntotal_losses = []\niterations = []","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Tried to use Gradient Clipping Since the loss is oscillating too much\ndef adaptive_clip_grad(parameters, clip_factor=0.01, epsilon=1e-3):\n    for p in parameters:\n        if p.grad is None:\n            continue\n        param_norm = torch.norm(p, p=2).clamp(min=epsilon)  # Avoid div by zero\n        grad_norm = torch.norm(p.grad, p=2)\n        clip_val = clip_factor * param_norm\n        p.grad *= (clip_val / grad_norm).clamp(max=1.0)  # Scale if needed","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport random\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\n\ndef tensor_to_image(tensor):\n    # Move tensor to CPU and convert to NumPy\n    image = tensor.detach().clone().cpu().numpy()\n\n    # Reshape the image to (C, H, W) from (1, C, H, W)\n    image = image.squeeze(0)\n\n    # Denormalize the image\n    image = image.transpose(1, 2, 0)\n    return image\n    \ndef save_one_test(model,i):\n    # Define preprocessing transforms\n    test_tf = transforms.Compose([\n        transforms.Resize((256, 256)),  # Resize to (512, 512)\n        transforms.ToTensor(),  # Convert image to tensor\n    ])\n    \n    \n    \n    # Select two different random images\n    content_idx = random.randint(0, len(content_dataset) - 1)\n    style_idx = random.randint(0, len(style_dataset) - 1)\n    \n    \n    content_image = content_dataset[content_idx]  # Get content image\n    style_image = style_dataset[style_idx]  # Get style image\n    \n    # Move images to device (CPU/GPU)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    content_image = content_image.unsqueeze(0).to(device)  # Add batch dimension\n    style_image = style_image.unsqueeze(0).to(device)\n    \n    # Perform inference with no gradient computation\n    with torch.no_grad():\n        output = model.decoder(adain(model.encode(content_image), model.encode(style_image)))\n        \n    # Convert tensors to images\n    content_img = tensor_to_image(content_image)\n    style_img = tensor_to_image(style_image)\n    output_img = tensor_to_image(output)\n    \n    # Display all images side by side\n    plt.figure(figsize=(15, 5))\n    \n    plt.subplot(1, 3, 1)\n    plt.imshow(content_img)\n    plt.title(\"Content Image\")\n    plt.axis(\"off\")\n    \n    plt.subplot(1, 3, 2)\n    plt.imshow(style_img)\n    plt.title(\"Style Image\")\n    plt.axis(\"off\")\n    \n    plt.subplot(1, 3, 3)\n    plt.imshow(output_img)\n    plt.title(\"Stylized Output\")\n    plt.axis(\"off\")\n    \n    plt.savefig(f\"output{i}.png\", dpi=300, bbox_inches='tight')\n    plt.close()\nsave_one_test(model,1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#train loop\nfrom tqdm import tqdm\nmodel.load_state_dict(torch.load(\"/kaggle/input/trained_1.3l/pytorch/default/1/network_130000.pth\"))\nmodel.train()\nfor i in range(max_iter+1):\n    optimizer.zero_grad()\n    i=i+130000\n    adjust_learning_rate(optimizer, iteration_count=i)\n    content_image=next(content_iter).to(device)\n    style_image=next(style_iter).to(device)\n    content_loss,style_loss=model(content_image,style_image)\n    loss=content_weight*content_loss+style_weight*style_loss\n    loss.backward()\n    # adaptive_clip_grad(model.decoder.parameters())\n    optimizer.step()\n    # model.update_ema()\n    \n    if i%50==0:\n        # Store loss values\n        content_losses.append(content_loss.item())\n        style_losses.append(style_loss.item())\n        total_losses.append(loss.item())\n        iterations.append(i)\n    \n    if i%100==0:\n        print(f'Iteration: {i}, Content Loss: {content_loss.item()}, Style Loss: {style_loss.item()}')\n        save_one_test(model,i)\n    if i%save_iter==0 or i+1==max_iter:\n        torch.save(model.state_dict(), f'network_{i}.pth')\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set model to evaluation mode\nmodel.eval()\ndef show_one_test(model,i):\n    # Define preprocessing transforms\n    test_tf = transforms.Compose([\n        transforms.Resize((256, 256)),  # Resize to (512, 512)\n        transforms.ToTensor(),  # Convert image to tensor\n    ])\n    \n    \n    \n    # Select two different random images\n    content_idx = random.randint(0, len(content_dataset) - 1)\n    style_idx = random.randint(0, len(style_dataset) - 1)\n    \n    \n    content_image = content_dataset[content_idx]  # Get content image\n    style_image = style_dataset[style_idx]  # Get style image\n    \n    # Move images to device (CPU/GPU)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    content_image = content_image.unsqueeze(0).to(device)  # Add batch dimension\n    style_image = style_image.unsqueeze(0).to(device)\n    \n    # Perform inference with no gradient computation\n    with torch.no_grad():\n        output = model.decoder(adain(model.encode(content_image), model.encode(style_image)))\n        \n    # Convert tensors to images\n    content_img = tensor_to_image(content_image)\n    style_img = tensor_to_image(style_image)\n    output_img = tensor_to_image(output)\n    \n    # Display all images side by side\n    plt.figure(figsize=(15, 5))\n    \n    plt.subplot(1, 3, 1)\n    plt.imshow(content_img)\n    plt.title(\"Content Image\")\n    plt.axis(\"off\")\n    \n    plt.subplot(1, 3, 2)\n    plt.imshow(style_img)\n    plt.title(\"Style Image\")\n    plt.axis(\"off\")\n    \n    plt.subplot(1, 3, 3)\n    plt.imshow(output_img)\n    plt.title(\"Stylized Output\")\n    plt.axis(\"off\")\n    \n    plt.show()\n    plt.close()\nshow_one_test(model,-1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#plotting the loss function\n\n# Plot Loss Graph\nplt.figure(figsize=(10, 5))\nplt.plot(iterations, content_losses, label=\"Content Loss\", color='blue')\nplt.plot(iterations, style_losses, label=\"Style Loss\", color='red')\nplt.plot(iterations, total_losses, label=\"Total Loss\", color='green')\nplt.xlabel(\"Iterations\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training Losses Over Iterations\")\nplt.legend()\nplt.grid()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}